# BiLSTM-using-2-decoder
This project implements a BiLSTM model with tokenization, incorporating two decoders for sequence-to-sequence tasks. The model utilizes early stopping to prevent overfitting and is optimized for accuracy. Tokenized input sequences are fed into the BiLSTM layers, processed through two distinct decoders, and performance is measured based on accuracy.
